{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame,Series\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import confusion_matrix,auc,roc_auc_score\n",
    "from sklearn.metrics import recall_score, precision_score, accuracy_score, f1_score\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('attrition.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Gender: ', df.Gender.unique() )\n",
    "print('BusinessTravel: ', df.BusinessTravel.unique() )\n",
    "print('Department: ', df.Department.unique() )\n",
    "print('EducationField: ', df.EducationField.unique() )\n",
    "print('JobRole: ', df.JobRole.unique() )\n",
    "print('MaritalStatus: ', df.MaritalStatus.unique() )\n",
    "print('OverTime: ', df.OverTime.unique() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('Attrition').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Gender'].replace(['Female','Male'], [0 , 1], inplace = True)\n",
    "df['BusinessTravel'].replace(['Travel_Rarely','Travel_Frequently','Non-Travel'], [0 , 1 , 2], inplace = True)\n",
    "df['Department'].replace(['Sales' , 'RandD', 'HR'], [0 , 1 , 2], inplace = True)\n",
    "df['EducationField'].replace(['Life_SC','Other','Medical','Marketing','TECH','EDU_HR'], [0,1,2,3,4,5], inplace = True)\n",
    "df['JobRole'].replace(['Sales_exec','Scientist','Lab_tech','mfg_director','health_rep','Manager',\n",
    " 'Sales_rep','Research_dir','Job_HR'], [0,1,2,3,4,5,6,7,8], inplace = True)\n",
    "df['MaritalStatus'].replace(['Single','Married','Divorced'], [0,1,2], inplace = True)\n",
    "df['OverTime'].replace(['Yes','No'], [0 , 1], inplace = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduntant Column Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are not able to visualize clearly. So, we are going for different apporach of finding correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code for getting larger figure of correlation\n",
    "cmap=sns.diverging_palette(5, 250, as_cmap=True)\n",
    "\n",
    "def magnify():\n",
    "    return [dict(selector=\"th\",\n",
    "                 props=[(\"font-size\", \"7pt\")]),\n",
    "            dict(selector=\"td\",\n",
    "                 props=[('padding', \"0em 0em\")]),\n",
    "            dict(selector=\"th:hover\",\n",
    "                 props=[(\"font-size\", \"12pt\")]),\n",
    "            dict(selector=\"tr:hover td:hover\",\n",
    "                 props=[('max-width', '200px'),\n",
    "                        ('font-size', '12pt')])\n",
    "]\n",
    "corr=df.corr()\n",
    "corr.style.background_gradient(cmap, axis=1)\\\n",
    "    .set_properties(**{'max-width': '80px', 'font-size': '10pt'})\\\n",
    "    .set_caption(\"Hover to magify\")\\\n",
    "    .set_precision(2)\\\n",
    "    .set_table_styles(magnify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While comparing we didnot get any of the correlation > 0.90. So, it is better to keep all columns. As because none of the columns is in high relation with Attrition so chances of bring collinearity is minimum.\n",
    "Here, we can find certain correlation among the variable as :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Age                vs    TotalWorkingYears          AS   68%                                                                   JobLevel           vs    MonthlyIncome              As   95%\n",
    "    JobLevel           vs    TotalWorkingYears          As   78%\n",
    "    MaritalStatus      vs    StockObtainLevel           As   66%\n",
    "    MonthlyIncome      vs    TotalWorkingYears          As   77%\n",
    "    PercentSalaryHike  vs    PerformanceRating          As   77%\n",
    "    YearsAtCompany     vs    YearsInCurrentRole         As   76%\n",
    "    YearsAtCompany     vs    YearsSinceLastPromotion    As   62%\n",
    "    YearsAtCompany     vs    YearsWithCurrManager       As   77%\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are considering approx 80 percent correlation as good correlation.\n",
    "So, we are left with:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    JobLevel           vs    MonthlyIncome              As   95%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can drop JobLevel, MonthlyIncome , PercentSalaryHike , YearsAtCompany."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropping columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['JobLevel'], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversion to categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Attrition'] = df['Attrition'].astype('category')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Gender'] = df['Gender'].astype('category')\n",
    "df['EducationField'] = df['EducationField'].astype('category')\n",
    "df['JobRole'] = df['JobRole'].astype('category')\n",
    "df['Department'] = df['Department'].astype('category')\n",
    "df['MaritalStatus'] = df['MaritalStatus'].astype('category')\n",
    "# df['OverTime'] = df['OverTime'].astype('category')\n",
    "df['BusinessTravel'] = df['BusinessTravel'].astype('category')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='Attrition', y='Age', data=df)\n",
    "plt.title('Age Vs Attrition')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='Attrition', y='DailyRate', data=df)\n",
    "plt.title('Attrition Vs DailyRate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(pd.crosstab(df.Attrition , df.BusinessTravel,  normalize = 'columns'),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                             0 - Travel-Rarely\n",
    "                                             1 - Travel-Frequently\n",
    "                                             2 - Non-Travel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='Attrition', y='DistanceFromHome', data=df)\n",
    "plt.title('Attrition Vs DistanceFromHome')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACC_perct = round(pd.crosstab(df.Attrition , df.JobRole,  normalize = 'columns'),2)\n",
    "print(ACC_perct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    0 - 'Sales_exec'\n",
    "    1 - 'Scientist'\n",
    "    2 - 'Lab_tech'\n",
    "    3 - 'mfg_director'\n",
    "    4 - 'health_rep'\n",
    "    5 - 'Manager'\n",
    "    6 - 'Sales_rep'\n",
    "    7 - 'Research_dir'\n",
    "    8 - 'Job_HR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACC_perct = round(pd.crosstab(df.Attrition , df.MaritalStatus,  normalize = 'columns'),2)\n",
    "print(ACC_perct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    0- 'Single'\n",
    "    1- 'Married'\n",
    "    2- 'Divorced'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACC_perct = round(pd.crosstab(df.Attrition , df.OverTime,  normalize = 'columns'),2)\n",
    "print(ACC_perct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                0 - 'Yes'\n",
    "                                1 - 'No'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='Attrition', y='TotalWorkingYears', data=df)\n",
    "plt.title('Attrition Vs TotalWorkingYears')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='Attrition', y='YearsInCurrentRole', data=df)\n",
    "plt.title('Attrition Vs YearsInCurrentRole')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='Attrition', y='YearsWithCurrManager', data=df)\n",
    "plt.title('Attrition Vs YearsWithCurrManager')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spilitting of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dataframes for X and Y variables\n",
    "x = df.drop([\"Attrition\"], axis=1)\n",
    "y = df[['Attrition']]\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Convert x to dummy variables\n",
    "x=pd.get_dummies(x , drop_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.30, random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape,X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model1:- Criterion1='gini'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gini=DecisionTreeClassifier()\n",
    "model_gini.fit(X_train, y_train)\n",
    "preds_gini = model_gini.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gini_train=DecisionTreeClassifier()\n",
    "model_gini_train.fit(X_train, y_train)\n",
    "preds_gini_train= model_gini.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion matrix\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "mat_gini = confusion_matrix(y_test,preds_gini)\n",
    "\n",
    "print(\"confusion matrix = \\n\",mat_gini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate accuracy\n",
    "print(accuracy_score(y_test,preds_gini))\n",
    "print(accuracy_score(y_train,preds_gini_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_test,preds_gini))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model1:- Criterion2='entropy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_entropy=DecisionTreeClassifier(criterion='entropy')\n",
    "model_entropy.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_entropy = model_entropy.predict(X_test)\n",
    "preds_entropy_train = model_entropy.predict(X_train)\n",
    "#Confusion matrix\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "mat_gini = confusion_matrix(y_test,preds_entropy)\n",
    "\n",
    "print(\"confusion matrix = \\n\",mat_gini)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(y_test,preds_entropy))\n",
    "print(accuracy_score(y_train,preds_entropy_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It is an overfit model as because the acc of train > acc of test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_test,preds_entropy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion matrix\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "mat_entropy = confusion_matrix(y_test,preds_entropy)\n",
    "\n",
    "print(\"confusion matrix = \\n\",mat_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "#from sklearn.externals.six import StringIO  \n",
    "from six import StringIO\n",
    "from IPython.display import Image  \n",
    "import pydotplus\n",
    "import graphviz\n",
    "from graphviz import Digraph\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "feature_cols = x.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree from entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = StringIO()\n",
    "export_graphviz(model_gini, out_file=dot_data,  \n",
    "                filled=True, rounded=True,\n",
    "                special_characters=True,feature_names = feature_cols,class_names=['0','1'])\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "graph.write_png('delay_gini.png')\n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The tree is overgrown due to which there is overfitting - we will now prune the tree and re-evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_pruned = DecisionTreeClassifier(criterion = \"gini\", random_state = 100,\n",
    "                               max_depth=5, min_samples_leaf=5)\n",
    "clf_pruned.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = StringIO()\n",
    "export_graphviz(clf_pruned, out_file=dot_data,  \n",
    "                filled=True, rounded=True,\n",
    "                special_characters=True,feature_names = feature_cols,class_names=['0','1'])\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "graph.write_png('delay_pruned.png')\n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_pruned = clf_pruned.predict(X_test)\n",
    "preds_pruned_train = clf_pruned.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion matrix\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "mat_pruned = confusion_matrix(y_test,preds_pruned)\n",
    "\n",
    "print(\"confusion matrix = \\n\",mat_pruned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(y_test,preds_pruned))\n",
    "print(accuracy_score(y_train,preds_pruned_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_test,preds_pruned))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_importance = clf_pruned.tree_.compute_feature_importances(normalize=False)\n",
    "print(\"feat importance = \" + str(feat_importance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_imp_dict = dict(zip(feature_cols, clf_pruned.feature_importances_))\n",
    "feat_imp = pd.DataFrame.from_dict(feat_imp_dict, orient='index')\n",
    "feat_imp.sort_values(by=0, ascending=False).head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the first 10 predicted probabilities of class membership\n",
    "clf_pruned.predict_proba(X_test)[0:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the predicted probabilities for class 1\n",
    "y_pred_prob = clf_pruned.predict_proba(X_test)[:, 1]\n",
    "y_pred_prob[1:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of predicted probabilities\n",
    "\n",
    "# 8 bins\n",
    "plt.hist(y_pred_prob, bins=20)\n",
    "\n",
    "# x-axis limit from 0 to 1\n",
    "plt.xlim(0,1)\n",
    "plt.title('Histogram of predicted probabilities')\n",
    "plt.xlabel('Predicted probability of diabetes')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Changing the cut off value for prediction\n",
    "pred_proba_df = pd.DataFrame(clf_pruned.predict_proba(X_test))\n",
    "threshold_list = [0.01,0.02,0.03,0.04,0.05,0.1,0.15,0.17,0.19,0.2,0.25,0.3,0.35,0.38,0.4,0.41,0.42,0.43,0.45,.5,0.6,.7,.8,.9,.99]\n",
    "for i in threshold_list:\n",
    "    print ('\\n******** For i = {} ******'.format(i))\n",
    "    y_test_pred = pred_proba_df.applymap(lambda x:1 if x>i else 0)\n",
    "    test_accuracy = metrics.accuracy_score(y_test.values.reshape(y_test.values.size,1),\n",
    "                                           y_test_pred.iloc[:,1].values.reshape(y_test_pred.iloc[:,1].values.size,1))\n",
    "    print('Our testing accuracy is {:.2f}'.format(test_accuracy))\n",
    "\n",
    "    print(confusion_matrix(y_test.values.reshape(y_test.values.size,1),\n",
    "                           y_test_pred.iloc[:,1].values.reshape(y_test_pred.iloc[:,1].values.size,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allow plots to appear in the notebook\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# adjust the font size \n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc, make_scorer, recall_score, accuracy_score, precision_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [3, 4, 5,6,7,8],\n",
    "    }\n",
    "\n",
    "scorers = {\n",
    "    'precision_score': make_scorer(precision_score),\n",
    "    'recall_score': make_scorer(recall_score),\n",
    "    'accuracy_score': make_scorer(accuracy_score)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_wrapper(refit_score='precision_score'):\n",
    "    \"\"\"\n",
    "    fits a GridSearchCV classifier using refit_score for optimization\n",
    "    prints classifier performance metrics\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=10)\n",
    "    grid_search = GridSearchCV(clf, param_grid, scoring=scorers, refit=refit_score,\n",
    "                           cv=skf, return_train_score=True, n_jobs=-1)\n",
    "    grid_search.fit(X_train.values, y_train.values)\n",
    "\n",
    "    # make the predictions\n",
    "    y_pred = grid_search.predict(X_test.values)\n",
    "\n",
    "    print('Best params for {}'.format(refit_score))\n",
    "    print(grid_search.best_params_)\n",
    "\n",
    "    # confusion matrix on the test data.\n",
    "    print('\\nConfusion matrix of optimized for {} on the test data:'.format(refit_score))\n",
    "    print(pd.DataFrame(confusion_matrix(y_test, y_pred),\n",
    "                 columns=['pred_neg', 'pred_pos'], index=['neg', 'pos']))\n",
    "    return grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_clf = grid_search_wrapper(refit_score='recall_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(grid_search_clf.cv_results_)\n",
    "results = results.sort_values(by='mean_test_precision_score', ascending=False)\n",
    "results[['mean_test_precision_score', 'mean_test_recall_score', 'mean_test_accuracy_score', 'param_max_depth']].round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores = grid_search_clf.predict_proba(X_test)[:, 1]\n",
    "# for classifiers with decision_function, this achieves similar results\n",
    "# y_scores = classifier.decision_function(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, r, thresholds = precision_recall_curve(y_test, y_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjusted_classes(y_scores, t):\n",
    "    \"\"\"\n",
    "    This function adjusts class predictions based on the prediction threshold (t).\n",
    "    Will only work for binary classification problems.\n",
    "    \"\"\"\n",
    "    return [1 if y >= t else 0 for y in y_scores]\n",
    "\n",
    "def precision_recall_threshold(p, r, thresholds, t=0.5):\n",
    "    \"\"\"\n",
    "    plots the precision recall curve and shows the current value for each\n",
    "    by identifying the classifier's threshold (t).\n",
    "    \"\"\"\n",
    "    \n",
    "    # generate new class predictions based on the adjusted_classes\n",
    "    # function above and view the resulting confusion matrix.\n",
    "    y_pred_adj = adjusted_classes(y_scores, t)\n",
    "    print(pd.DataFrame(confusion_matrix(y_test, y_pred_adj),\n",
    "                       columns=['pred_neg', 'pred_pos'], \n",
    "                       index=['neg', 'pos']))\n",
    "    \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n",
    "    \"\"\"\n",
    "    Modified from:\n",
    "    Hands-On Machine learning with Scikit-Learn\n",
    "    and TensorFlow; p.89\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.title(\"Precision and Recall Scores as a function of the decision threshold\")\n",
    "    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n",
    "    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.xlabel(\"Decision Threshold\")\n",
    "    plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the same p, r, thresholds that were previously calculated\n",
    "plot_precision_recall_vs_threshold(p, r, thresholds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model building with threshold value 0.19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_pruned_final = DecisionTreeClassifier(criterion = \"gini\", random_state = 100,\n",
    "                               max_depth=8, min_samples_leaf=5)\n",
    "clf_pruned_final.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = StringIO()\n",
    "export_graphviz(clf_pruned_final, out_file=dot_data,  \n",
    "                filled=True, rounded=True,\n",
    "                special_characters=True,feature_names = feature_cols,class_names=['0','1'])\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "graph.write_png('delay_pruned.png')\n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_pruned_final = clf_pruned_final.predict(X_test)\n",
    "preds_pruned_train1 = clf_pruned_final.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion matrix\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "mat_pruned_final = confusion_matrix(y_test,preds_pruned_final)\n",
    "\n",
    "print(\"confusion matrix = \\n\",mat_pruned_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Changing the cut off value for prediction\n",
    "pred_proba = pd.DataFrame(clf_pruned_final.predict_proba(X_test))\n",
    "threshold_list = [0.19]\n",
    "for i in threshold_list:\n",
    "    print ('\\n******** For i = {} ******'.format(i))\n",
    "    y_test_pred = pred_proba.applymap(lambda x:1 if x>i else 0)\n",
    "    test_accurac = metrics.accuracy_score(y_test.as_matrix().reshape(y_test.as_matrix().size,1),\n",
    "                                           y_test_pred.iloc[:,1].as_matrix().reshape(y_test_pred.iloc[:,1].as_matrix().size,1))\n",
    "    print('Our testing a\n",
    "          ccuracy is {:.2f}'.format(test_accurac))\n",
    "\n",
    "    print(confusion_matrix(y_test.as_matrix().reshape(y_test.as_matrix().size,1),\n",
    "                           y_test_pred.iloc[:,1].as_matrix().reshape(y_test_pred.iloc[:,1].as_matrix().size,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(y_test,preds_pruned_final))\n",
    "print(accuracy_score(y_train,preds_pruned_train1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_test,preds_pruned))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_importance1 = clf_pruned.tree_.compute_feature_importances(normalize=False)\n",
    "print(\"feat importance = \" + str(feat_importance1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features affecting employees staying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_imp_dict = dict(zip(feature_cols, clf_pruned.feature_importances_))\n",
    "feat_imp = pd.DataFrame.from_dict(feat_imp_dict, orient='index')\n",
    "feat_imp.sort_values(by=0, ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = clf_pruned_final.predict_proba(x)[:, 1]\n",
    "y_pred_proba[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding probabilities of Attrition to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w=pd.Series(y_pred_proba)\n",
    "df['y_pred_proba']=w\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
